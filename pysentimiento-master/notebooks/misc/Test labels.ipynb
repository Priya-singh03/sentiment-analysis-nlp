{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4802 2443 7264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Index(['text', 'polarity', 'lang', 'label'], dtype='object'),\n",
       " Index(['text', 'polarity', 'lang', 'label'], dtype='object'),\n",
       " Index(['text', 'polarity', 'lang', 'label'], dtype='object'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "def get_lang(file):\n",
    "    return os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "\"\"\"\n",
    "Lo pongo así por hugginface\n",
    "\"\"\"\n",
    "id2label = {0: 'N', 1: 'NEU', 2: 'P'}\n",
    "label2id = {v:k for k,v in id2label.items()}\n",
    "\n",
    "\n",
    "def get_lang(file):\n",
    "    \"\"\"\n",
    "    Get language of TASS dataset\n",
    "    \"\"\"\n",
    "    return os.path.splitext(os.path.basename(file))[0]\n",
    "\n",
    "def load_df(path, test=False):\n",
    "    \"\"\"\n",
    "    Load TASS dataset\n",
    "    \"\"\"\n",
    "    dialect = get_lang(path)\n",
    "    \n",
    "    if test:\n",
    "        df = pd.read_table(path, names=[\"id\", \"text\"], index_col=0)\n",
    "        label_path = os.path.join(\n",
    "            os.path.dirname(path),\n",
    "            \"labels\",\n",
    "            f\"{dialect.lower()}.tsv\"\n",
    "        )\n",
    "        \n",
    "        labels = pd.read_table(label_path, names=[\"id\", \"label\"], index_col=0)\n",
    "\n",
    "        df[\"polarity\"] = labels[\"label\"]\n",
    "    else:\n",
    "        df = pd.read_table(path, names=[\"id\", \"text\", \"polarity\"], index_col=0)\n",
    "    \n",
    "    df[\"lang\"] = dialect\n",
    "\n",
    "    for label, idx in label2id.items():\n",
    "        df.loc[df[\"polarity\"] == label, \"label\"] = idx\n",
    "    return df\n",
    "\n",
    "\n",
    "train_files = glob(\"../../data/tass2020/train/*.tsv\")\n",
    "dev_files = glob(\"../../data/tass2020/dev/*.tsv\")\n",
    "test_files = glob(\"../../data/tass2020/test1.1/*.tsv\")\n",
    "\n",
    "\n",
    "train_dfs = {get_lang(file):load_df(file) for file in train_files}\n",
    "dev_dfs = {get_lang(file):load_df(file) for file in dev_files}\n",
    "test_dfs = {get_lang(file):load_df(file, test=True) for file in test_files}\n",
    "\n",
    "train_df = pd.concat(train_dfs.values())\n",
    "dev_df = pd.concat(dev_dfs.values())\n",
    "test_df = pd.concat(test_dfs.values())\n",
    "\n",
    "print(len(train_df), len(dev_df), len(test_df))\n",
    "\n",
    "train_df.columns, dev_df.columns, test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/tass2020/test1.1/CR.tsv'\n",
    "df = load_df(path)\n",
    "\n",
    "labels = pd.read_table('../../data/tass2020/test1.1/labels/cr.tsv', names=[\"id\", \"label\"], index_col=0)\n",
    "\n",
    "df[\"polarity\"] = labels[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train_df[\"label\"].isna())a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Convertimos todos los usuarios al string \"usuario\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(preprocess_tweet)\n",
    "dev_df[\"text\"] = dev_df[\"text\"].apply(preprocess_tweet)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(preprocess_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e65cc673ee40c8812cea0866dd1152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd7bb6e82994ad4b84044285055f95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=441944381.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df169f42595244abbdd5d0a1ce161d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242120.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d547f682d3c541bf89e1b8a9e42dcbed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc395851d66c406bb6db0cdc702e99ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15d0ceae1d341c995ceefcb55b8b8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=43.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, return_dict=True, num_labels=3)\n",
    "\n",
    "model.config.hidden_dropout_prob = 0.20\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "model = model.to(device)\n",
    "model.train();\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({0: 'N', 1: 'NEU', 2: 'P'}, {'N': 0, 'NEU': 1, 'P': 2})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label, model.config.label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos primero las longitudes (a ver si no hay nada mal cargado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "\n",
    "examples = pd.concat([train_df, dev_df])\n",
    "\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "    'label': ClassLabel(num_classes=3, names=[\"neg\", \"neu\", \"pos\"])\n",
    "})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label\"]], features=features)\n",
    "dev_dataset = Dataset.from_pandas(dev_df[[\"text\", \"label\"]], features=features)\n",
    "test_dataset = Dataset.from_pandas(test_df[[\"text\", \"label\"]], features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=3, names=['neg', 'neu', 'pos'], names_file=None, id=None)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce13689e18f140f48d5aa08aa0c26f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b8d5d7537f9442993345e277df6bcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=153.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dd7f17c8604d4bade2d2dcaea27bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=454.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "eval_batch_size = 16\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "dev_dataset = dev_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e386de0e63a14e319beacefed7194a39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4802.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3274e44d4fba47529205818b39949497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2443.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9041a3d873b46689bd48e27fac3e5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7264.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def format_dataset(dataset):\n",
    "    dataset = dataset.map(lambda examples: {'labels': examples['label']})\n",
    "    dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n",
    "    return dataset\n",
    "\n",
    "train_dataset = format_dataset(train_dataset)\n",
    "dev_dataset = format_dataset(dev_dataset)\n",
    "test_dataset = format_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext import data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_it = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
    "dev_it = torch.utils.data.DataLoader(dev_dataset, batch_size=batch_size)\n",
    "test_it = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def evaluate(model, it):\n",
    "    \"\"\"\n",
    "    Calculates labels and predictions for it\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    (labels, preds): torch.Tensor\n",
    "    \n",
    "    where labels are the true labels\n",
    "    and preds are the predictions\n",
    "    \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true = []\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(it)):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            losses.append(loss.item())\n",
    "            outs = torch.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "            true.append(batch['labels'].cpu())\n",
    "            preds.append(outs.cpu())\n",
    "    return np.array(losses).mean(), torch.cat(true), torch.cat(preds).argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5fe4843a4049fdbf0e70fe84cae59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7d4005c81c4b9ba4b74788e50ce1b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0 \n",
      "Train loss 0.9431\n",
      "Dev loss 0.8937\n",
      "Dev metrics {'accuracy': 0.5906672124437168, 'f1': 0.5505197996698029, 'precision': 0.6106491589125013, 'recall': 0.5665948890128735}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0468141f382148f1a743fa519c790387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5bb3d881184953822335ea913e3764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 \n",
      "Train loss 0.6707\n",
      "Dev loss 0.7980\n",
      "Dev metrics {'accuracy': 0.6577977896029472, 'f1': 0.6492644936984707, 'precision': 0.6476824515764337, 'recall': 0.6580308478017977}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "854fc99d99a6443ab9b2bd6fdcb9c284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2479b9eb89843368d2b0e98526cc717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 \n",
      "Train loss 0.4710\n",
      "Dev loss 0.8357\n",
      "Dev metrics {'accuracy': 0.664756446991404, 'f1': 0.655528088673364, 'precision': 0.6546657428709367, 'recall': 0.6621937802322934}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa85b6bf67847ada8fde1f9c2da0273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "960d44100cd346088fcc644554825b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 \n",
      "Train loss 0.3385\n",
      "Dev loss 0.8777\n",
      "Dev metrics {'accuracy': 0.6643471142038477, 'f1': 0.6557570313553263, 'precision': 0.6562553770602814, 'recall': 0.6590732953302344}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe185c121b094781ba200feca69e7d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=151.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9797615eb1794a2497b8f669bf40cc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=77.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 \n",
      "Train loss 0.2560\n",
      "Dev loss 0.9205\n",
      "Dev metrics {'accuracy': 0.6532951289398281, 'f1': 0.658762960650657, 'precision': 0.6780016950486027, 'recall': 0.6543953935702859}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.train().to(device)\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "num_training_steps = epochs * len(train_it)\n",
    "num_warmup_steps = num_training_steps // 10\n",
    "warmup_proportion = float(num_warmup_steps) / float(num_training_steps)  # 0.1\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_losses = []\n",
    "    for i, batch in enumerate(tqdm(train_it)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    train_loss = np.array(train_losses).mean()\n",
    "    dev_loss, dev_labels, dev_preds = evaluate(model, dev_it)\n",
    "    dev_metrics = compute_metrics(dev_labels, dev_preds) \n",
    "    \n",
    "    print(f\"Epoch {epoch:<2}\")\n",
    "    print(f\"Train loss {train_loss:.4f}\")\n",
    "    print(f\"Dev loss {dev_loss:.4f}\")\n",
    "    print(f\"Dev metrics {dev_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1:\n",
    "\n",
    "1. 0.6490934045540707\n",
    "2. 0.6565782147866169\n",
    "3. 0.6595779895659064\n",
    "4. 0.6610761387899147\n",
    "5. 0.658762960650657\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.6570), tensor(0.0047))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.Tensor([\n",
    "    0.6490934045540707,\n",
    "    0.6565782147866169,\n",
    "    0.6595779895659064,\n",
    "    0.6610761387899147,\n",
    "    0.658762960650657,\n",
    "])\n",
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label[0] = \"NEG\"\n",
    "model.config.id2label[2] = \"POS\"\n",
    "\n",
    "model.config.label2id = {v:k for k,v in model.config.id2label.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../models/beto-sentiment-analysis/vocab.txt',\n",
       " '../models/beto-sentiment-analysis/special_tokens_map.json',\n",
       " '../models/beto-sentiment-analysis/added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../models/beto-sentiment-analysis\"\n",
    "model.save_pretrained(path)\n",
    "tokenizer.save_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
