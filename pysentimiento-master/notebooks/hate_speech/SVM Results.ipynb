{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM results\n",
    "\n",
    "In this notebook we explore the *hatEval* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pysentimiento:Train path = /home/jmperez/projects/pysentimiento/data/hate/hateval2019_es_train.csv\n",
      "INFO:pysentimiento:Dev path = /home/jmperez/projects/pysentimiento/data/hate/hateval2019_es_dev.csv\n",
      "INFO:pysentimiento:Test path = /home/jmperez/projects/pysentimiento/data/hate/hateval2019_es_test.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "from datasets import concatenate_datasets\n",
    "from pysentimiento.hate import load_datasets\n",
    "\n",
    "train, dev, test = load_datasets(lang=\"es\", preprocess=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = test.to_pandas().set_index(\"id\")\n",
    "df_eval = pd.read_table(\"svm/es_a.tsv\", header=None, names=[\"id\", \"HS\"]).set_index(\"id\")\n",
    "\n",
    "df_test[\"HS_pred\"] = df_eval[\"HS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.82945736, 0.63922518]),\n",
       " array([0.68297872, 0.8       ]),\n",
       " array([0.74912485, 0.71063257]),\n",
       " 0.7298787124009228)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "\n",
    "prec, recall, f1, _ = precision_recall_fscore_support(df_test[\"HS\"], df_test[\"HS_pred\"])\n",
    "\n",
    "macro_f1 = f1.mean()\n",
    "\n",
    "prec, recall, f1, macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN ELMo\n",
    "\n",
    "Evaluaci√≥n sobre test\n",
    "1600/1600 [==============================] - 0s 289us/step\n",
    "\n",
    "Loss           : 0.5544\n",
    "\n",
    "Accuracy       : 0.7388\n",
    "\n",
    "Precision(1)   : 0.6609\n",
    "\n",
    "Precision(1)   : 0.8078\n",
    "\n",
    "Precision(avg) : 0.7343\n",
    "\n",
    "Recall(1)      : 0.7530\n",
    "\n",
    "Recall(0)      : 0.7287\n",
    "\n",
    "Recall(avg)    : 0.7409\n",
    "\n",
    "\n",
    "F1(1)          : 0.7040\n",
    "\n",
    "F1(0)          : 0.7662\n",
    "\n",
    "F1(avg)        : 0.7351"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "387abc9bc94d8eb1bd0148a5d4cb2bf99bc3b40fa501b808e2b508b4f65ed831"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('pysentimiento-bwlKzHxB-py3.7': poetry)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
